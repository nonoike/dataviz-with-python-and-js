{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクレイピング\n",
    "Webからプログラムで入手するようには設計されていないデータを取得する行為"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- requests: HTTPリクエストを行うライブラリ\n",
    "- BeautifulSoup: 簡単なスクレイピングに便利\n",
    "  - lxml: BeautifulSoupのデフォルトパーサ、高速に動作\n",
    "- Scrapy: 大規模なデータスクレイピングに便利"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "# httpheaderに「User-Agent」属性を追加しないと、 Wikipediaはリクエストを拒否する\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ページ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (4.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Nobel_soup():\n",
    "    \"\"\" ノーベル賞ページの解析したタグツリーを返す \"\"\"\n",
    "    response = requests.get(URL, headers=HEADERS)\n",
    "    return BeautifulSoup(response.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_Nobel_soup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブル解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行結果の表示領域が大きいのでスキップ中\n",
    "# soup.find('table', {'class':'wikitable sortable'}) # 第1引数: タグ名, 第2引数: 各種属性のディクショナリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('table', {'class':'sortable wikitable'}) # クラス順番が異なる場合は空を返す → select 使え（スープの作成時に lxml パーサを指定すると使用可能）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行結果の表示領域が大きいのでスキップ中\n",
    "# soup.select('table.sortable.wikitable') # ← は複数返す、1つのみの場合は select_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitable = soup.select_one('table.sortable.wikitable')\n",
    "wikitable.select('th') == wikitable('th') # スープに対してタグを直接呼び出せる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_titles(table):\n",
    "    \"\"\" テーブルヘッダからノーベル賞分野を取得する \"\"\"\n",
    "    cols = []\n",
    "    for th in table.select_one('tr').select('th')[1:]: #  ヘッダーはスキップ\n",
    "        link = th.select_one('a')\n",
    "        \n",
    "        # 分野名と Wikipedia リンクを格納する\n",
    "        if link:\n",
    "            cols.append({'name': link.text, 'href': link.attrs['href']})\n",
    "        else:\n",
    "            cols.append({'name': th.text, 'href': None})\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Physics', 'href': '/wiki/List_of_Nobel_laureates_in_Physics'},\n",
       " {'name': 'Chemistry', 'href': '/wiki/List_of_Nobel_laureates_in_Chemistry'},\n",
       " {'name': 'Physiologyor Medicine',\n",
       "  'href': '/wiki/List_of_Nobel_laureates_in_Physiology_or_Medicine'},\n",
       " {'name': 'Literature', 'href': '/wiki/List_of_Nobel_laureates_in_Literature'},\n",
       " {'name': 'Peace', 'href': '/wiki/List_of_Nobel_Peace_Prize_laureates'},\n",
       " {'name': 'Economics', 'href': '/wiki/List_of_Nobel_laureates_in_Economics'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitable = soup.select_one('table.sortable.wikitable')\n",
    "get_column_titles(wikitable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### セル取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Nobel_winners(table):\n",
    "    cols = get_column_titles(table)\n",
    "    winners = []\n",
    "    for row in table.select('tr')[1:-1]:\n",
    "        year = int(row.select_one('td').text[:4]) # 2000[1]のような注釈付き対策\n",
    "        for i, td in enumerate(row.select('td')[1:]):\n",
    "            for winner in td.select('a'):\n",
    "                href = winner.attrs['href']\n",
    "                if not href.startswith('#endnote'):\n",
    "                    winners.append({ 'year':year,'category':cols[i]['name'], 'name':winner.text, 'link':winner.attrs['href']})\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'year': 1901,\n",
       "  'category': 'Physics',\n",
       "  'name': 'Wilhelm Röntgen',\n",
       "  'link': '/wiki/Wilhelm_R%C3%B6ntgen'},\n",
       " {'year': 1901,\n",
       "  'category': 'Chemistry',\n",
       "  'name': \"Jacobus Henricus van 't Hoff\",\n",
       "  'link': '/wiki/Jacobus_Henricus_van_%27t_Hoff'},\n",
       " {'year': 1901,\n",
       "  'category': 'Physiologyor Medicine',\n",
       "  'name': 'Emil Adolf von Behring',\n",
       "  'link': '/wiki/Emil_Adolf_von_Behring'},\n",
       " {'year': 1901,\n",
       "  'category': 'Literature',\n",
       "  'name': 'Sully Prudhomme',\n",
       "  'link': '/wiki/Sully_Prudhomme'},\n",
       " {'year': 1901,\n",
       "  'category': 'Peace',\n",
       "  'name': 'Henry Dunant',\n",
       "  'link': '/wiki/Henry_Dunant'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winners = get_Nobel_winners(wikitable)\n",
    "winners[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### キャッシュ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: requests-cache in /opt/conda/lib/python3.7/site-packages (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: requests>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from requests-cache) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=1.1.0->requests-cache) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=1.1.0->requests-cache) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=1.1.0->requests-cache) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=1.1.0->requests-cache) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade requests-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('nobel_pages', packend='sqlite', expire_after=7200) # nobel_pages という名前のキャッシュ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人物情報の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winner_nationality(w):\n",
    "    \"\"\" 受賞者の Wikipedia ページから人物情報データをスクレイピングする \"\"\"\n",
    "    url_str = 'http://en.wikipedia.org' + w['link']\n",
    "    data = requests.get(url_str, headers=HEADERS)\n",
    "    soup = BeautifulSoup(data.content, \"lxml\")\n",
    "    person_data = {'name': w['name']}\n",
    "    attr_rows = soup.select('table.infobox tr')\n",
    "    for tr in attr_rows:\n",
    "        try:\n",
    "            attribute = tr.select_one('th').text\n",
    "            if attribute == 'Nationality':\n",
    "                person_data[attribute] = tr.select_one('td').text\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return person_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_count = 10 # スクレイピングする回数(-1で全件)(国籍が存在しないなど取得に失敗しても1カウント)\n",
    "\n",
    "wdata = []\n",
    "for w in winners[:scraping_count]:\n",
    "    wdata.append(get_winner_nationality(w))\n",
    "    missing_nationality = []\n",
    "for w in wdata:\n",
    "    if not w.get('Nationality'):\n",
    "        missing_nationality.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Wilhelm Röntgen', 'Nationality': 'German[1]'},\n",
       " {'name': \"Jacobus Henricus van 't Hoff\", 'Nationality': 'Dutch'},\n",
       " {'name': 'Emil Adolf von Behring', 'Nationality': 'German'},\n",
       " {'name': 'Sully Prudhomme', 'Nationality': 'French'},\n",
       " {'name': 'Henry Dunant', 'Nationality': 'Swiss'},\n",
       " {'name': 'Frédéric Passy', 'Nationality': 'French'},\n",
       " {'name': 'Hendrik Lorentz', 'Nationality': 'Netherlands'},\n",
       " {'name': 'Pieter Zeeman', 'Nationality': 'Netherlands'},\n",
       " {'name': 'Hermann Emil Fischer', 'Nationality': 'Germany'},\n",
       " {'name': 'Ronald Ross', 'Nationality': 'British'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from scrapy) (4.3.3)\n",
      "Requirement already satisfied: pyOpenSSL in /opt/conda/lib/python3.7/site-packages (from scrapy) (19.0.0)\n",
      "Requirement already satisfied: Twisted>=13.1.0 in /opt/conda/lib/python3.7/site-packages (from scrapy) (19.2.0)\n",
      "Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.7/site-packages (from scrapy) (1.0.3)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.7/site-packages (from scrapy) (1.12.0)\n",
      "Requirement already satisfied: parsel>=1.5 in /opt/conda/lib/python3.7/site-packages (from scrapy) (1.5.1)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from scrapy) (1.20.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.7/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: queuelib in /opt/conda/lib/python3.7/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: service-identity in /opt/conda/lib/python3.7/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: cryptography>=2.3 in /opt/conda/lib/python3.7/site-packages (from pyOpenSSL->scrapy) (2.6.1)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (19.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (19.1.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (0.7.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (4.6.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (1.9.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.7/site-packages (from Twisted>=13.1.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity->scrapy) (0.4.5)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.7/site-packages (from service-identity->scrapy) (0.2.4)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.3->pyOpenSSL->scrapy) (0.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.3->pyOpenSSL->scrapy) (1.12.2)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->scrapy) (2.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->scrapy) (40.8.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL->scrapy) (2.19)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "if not os.path.exists('./nobel_winners'):\n",
    "    print('try `scrapy startproject`')\n",
    "    !scrapy startproject nobel_winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォルダ構成\n",
    "<pre>\n",
    "$ tree\n",
    ".\n",
    "├── nobel_winners\n",
    "│   ├── __init__.py\n",
    "│   ├── __pycache__\n",
    "│   ├── items.py\n",
    "│   ├── middlewares.py\n",
    "│   ├── pipelines.py\n",
    "│   ├── settings.py\n",
    "│   └── spiders\n",
    "│       ├── __init__.py\n",
    "│       └── __pycache__\n",
    "└── scrapy.cfg\n",
    "\n",
    "4 directories, 7 files\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scrapy shell` で対話的に操作できる\n",
    "- スパイダー: スクレイピング方法を定義するクラス\n",
    "  - `scrapy.Spider` クラスのサブクラスとして実装\n",
    "  - `spiders` ディレクトリに入れられたすべてのスパイダーは `Scrapy` が自動的に検出する → `scrapy` コマンドから名前でアクセス可\n",
    "- キャッシュを有効化させる場合は `${project}/settings.py` の `HTTPCACHE_ENABLED` を `True` に設定する\n",
    "- パイプライン: アイテムをスクレイピングした後に続けて行う処理\n",
    "  - 複数定義可\n",
    "  - Scrapy が定義済みのテンプレートのパイプラインもある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
